{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Deep Learning using PyTorch\n",
    "PyTorch is a Python package that allows for ease of creating ML models. It has a host of functions that make creating forward prop and back prop easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Basics\n",
    "tensors are the basic units that you work with when creatin ML models. They are like numpy arrays and represent matrices, which allow you to perform calculations very quickly with matrix math compared to creating many for-loops to run through. This section has some basic things that you can do with pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating some tensors for use in the following sections\n",
    "x = torch.rand(5,3)\n",
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5934, 0.9493, 1.1120],\n",
      "        [0.9743, 0.6347, 1.8129],\n",
      "        [1.3310, 1.9238, 1.6887],\n",
      "        [2.0307, 2.4931, 1.4076],\n",
      "        [0.7000, 2.0945, 0.4425]])\n",
      "tensor([[-0.6809, -0.4478, -0.2381],\n",
      "        [-0.3533, -0.5480, -0.3688],\n",
      "        [-0.6232, -0.5482, -0.3791],\n",
      "        [-0.7224, -0.6144, -0.1976],\n",
      "        [-0.5163, -0.6787, -0.3701]])\n",
      "tensor([[6.2260e-01, 2.8157e-02, 4.5461e-02],\n",
      "        [3.4062e-02, 1.0295e-03, 1.9226e-01],\n",
      "        [7.8057e-02, 2.5933e-01, 1.6254e-01],\n",
      "        [3.0911e-01, 5.4213e-01, 7.2321e-02],\n",
      "        [4.3589e-03, 3.4011e-01, 4.8568e-04]])\n",
      "tensor([[1.4685, 2.2331, 4.2003],\n",
      "        [2.8302, 1.8247, 2.7116],\n",
      "        [1.6047, 1.8242, 2.6377],\n",
      "        [1.3844, 1.6277, 5.0612],\n",
      "        [1.9370, 1.4735, 2.7021]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise and in-place addition\n",
    "a = torch.add(x, y)\n",
    "a.add_(x)\n",
    "\n",
    "# element-wise and in-place subtraction\n",
    "b = torch.sub(x, y)\n",
    "b.sub_(x)\n",
    "\n",
    "# element-wise and in-place multiplication\n",
    "c = torch.mul(x, y)\n",
    "c.mul_(x)\n",
    "\n",
    "# element-wise and in-place division\n",
    "d = torch.div(x, y)\n",
    "d.div_(x)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9562, 0.2508, 0.4370],\n",
      "        [0.3105, 0.0433, 0.7220],\n",
      "        [0.3539, 0.6878, 0.6548],\n",
      "        [0.6542, 0.9394, 0.6050],\n",
      "        [0.0919, 0.7079, 0.0362]])\n",
      "tensor([0.2508, 0.0433, 0.6878, 0.9394, 0.7079])\n",
      "tensor([0.3105, 0.0433, 0.7220])\n",
      "tensor(0.0433)\n"
     ]
    }
   ],
   "source": [
    "# using the same tensors created in the section before\n",
    "print(x)      # show the entire tensor\n",
    "print(x[:,1]) # gets the first column\n",
    "print(x[1,:]) # gets the first row\n",
    "print(x[1,1]) # gets the value at the (1,1) space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9562],\n",
      "        [0.2508],\n",
      "        [0.4370],\n",
      "        [0.3105],\n",
      "        [0.0433],\n",
      "        [0.7220],\n",
      "        [0.3539],\n",
      "        [0.6878],\n",
      "        [0.6548],\n",
      "        [0.6542],\n",
      "        [0.9394],\n",
      "        [0.6050],\n",
      "        [0.0919],\n",
      "        [0.7079],\n",
      "        [0.0362]])\n",
      "tensor([[0.6809, 0.4478, 0.2381, 0.3533, 0.5480],\n",
      "        [0.3688, 0.6232, 0.5482, 0.3791, 0.7224],\n",
      "        [0.6144, 0.1976, 0.5163, 0.6787, 0.3701]])\n"
     ]
    }
   ],
   "source": [
    "# use the view function - the product of the dimensions must be the same\n",
    "z = x.view(15, 1)\n",
    "print(z)\n",
    "z = y.view(-1, 5) # the -1 means 'whatever is left' - it will auto size\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from numpy ndarrays to pytorch tensors and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.593351   0.9493178  1.1120331 ]\n",
      " [0.9743099  0.6347309  1.8128557 ]\n",
      " [1.3310056  1.9237852  1.6886619 ]\n",
      " [2.030662   2.4931161  1.4075922 ]\n",
      " [0.7000372  2.0945072  0.44253206]]\n",
      "tensor([[2.5934, 0.9493, 1.1120],\n",
      "        [0.9743, 0.6347, 1.8129],\n",
      "        [1.3310, 1.9238, 1.6887],\n",
      "        [2.0307, 2.4931, 1.4076],\n",
      "        [0.7000, 2.0945, 0.4425]])\n"
     ]
    }
   ],
   "source": [
    "# from tensor to ndarray\n",
    "e = a.numpy() # both e and a point to the same tensor - so changing one will change the other\n",
    "print(e)\n",
    "\n",
    "# from ndarray to tensor\n",
    "a = torch.from_numpy(e) # both e and a point to the same tensor - so changing one will change the other\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "autograd is a package in pytorch that allows for automatically creating gradient descent in pytorch tensors. <br>\n",
    "gradient descent and gradients are central to creating ML models because this is how you train a model. Therefore, this package is central to the pytorch <br>\n",
    "<br>\n",
    "When performing an operation on a tensor using autograd, pytorch creates a computation graph which automatically stores a function that is the gradient (slope) of the node. This is convenient for use during back propogation because the function is already stored rather than it needing to be calculated every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.7854, grad_fn=<MeanBackward0>)\n",
      "tensor([2.8655, 1.9139, 6.1823])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True) # let the tensor know whether it can use autograd\n",
    "\n",
    "y = x + 2        # creates a computation graph - has an attribute that is the derivative of tensor + 2\n",
    "z = y * y * 2\n",
    "z = z.mean()\n",
    "print(z)         # the 'grad_fn' attribute is the gradient function\n",
    "\n",
    "z.backward()     # backward propogation dcomp/dtensor - tensor has attribute that stores grad values\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# with ML models, we need to calculate gradient descent - this is a simple example using a linear function\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    modelOutput = (weights * 3).sum()       # (weights * 3) is function, .sum() creates one attr so backwards() can be called\n",
    "    modelOutput.backward()\n",
    "    print(weights.grad)                     # if stop here, it will calculate the change from the starting position (ones)\n",
    "                                            # but, we want to calculate for each step, therefore, we reset the grads at each step\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent using Autograd\n",
    "gradient descent is the backbone of ML models - it makes the model more accurate - this is a quick intro on using gradient descent with pytorch autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "backpropogation is an essential step in deep learning. Here, based on the accuracy of the forward propogation step, you calculate the gradient descent and which set of parameters to test next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# initialize the input and expected result\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# initialize the weights\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute loss\n",
    "yhat = w * x\n",
    "loss = (yhat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backward pass and compute grads - computed automatically because we specified require_grad\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "In a typical ML model, you would throw all of this in a for loop and do it multiple times to make the prediction more accurate. This next portion will show that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using only numpy\n",
    "First, to ensure understanding of how these models work, implement using only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: x=5, y=4.260226058706072\n",
      "epoch: 1, w: 1.3204107207744176, loss: 33.60390090942383\n",
      "epoch: 2, w: 1.5976831206767614, loss: 11.776960372924805\n",
      "epoch: 3, w: 1.7618284148662147, loss: 4.127401828765869\n",
      "epoch: 4, w: 1.8590024108378944, loss: 1.4465053081512451\n",
      "epoch: 5, w: 1.9165294417827186, loss: 0.5069484710693359\n",
      "epoch: 6, w: 1.9505854491679724, loss: 0.17766721546649933\n",
      "epoch: 7, w: 1.970746562906986, loss: 0.062265701591968536\n",
      "epoch: 8, w: 1.982681985804325, loss: 0.021822046488523483\n",
      "epoch: 9, w: 1.9897477421252783, loss: 0.007647811435163021\n",
      "epoch: 10, w: 1.993930663058048, loss: 0.002680273959413171\n",
      "epoch: 11, w: 1.9964069473235662, loss: 0.0009393357904627919\n",
      "epoch: 12, w: 1.9978729242770725, loss: 0.00032921083038672805\n",
      "epoch: 13, w: 1.9987407638518864, loss: 0.00011537156387930736\n",
      "epoch: 14, w: 1.9992545232265049, loss: 4.043331500724889e-05\n",
      "epoch: 15, w: 1.9995586933581881, loss: 1.417271232639905e-05\n",
      "Prediction after training: x=5, y=9.997793466790942\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# linear regression so - function = weight * sample\n",
    "\n",
    "# initialize our training samples\n",
    "X = np.array([1,2,3,4,5,6,7,8], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8,10,12,14,16], dtype=np.float32)\n",
    "\n",
    "# initialize our weight\n",
    "w = random.random()\n",
    "\n",
    "# calculate model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# calculate loss - mean squared error\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# calculate gradient - the derivative of the loss with respect to the weights\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred-y).mean()\n",
    "\n",
    "# prediction before training\n",
    "print('Prediction before training: x=5, y=' + str(w*5))\n",
    "\n",
    "# training the model\n",
    "lr = 0.001\n",
    "n_iters = 15\n",
    "epochs = []\n",
    "losses = []\n",
    "weights = []\n",
    "for epoch in range(1, n_iters+1):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss(Y, y_pred)\n",
    "    epochs.append(epoch)\n",
    "    losses.append(l)\n",
    "\n",
    "    # update the weights\n",
    "    dW = gradient(X, Y, y_pred)\n",
    "    w -= lr * dW\n",
    "    weights.append(w)\n",
    "    \n",
    "    # update the epochs\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch}, w: {w}, loss: {l}')\n",
    "\n",
    "# print the loss\n",
    "plt.plot(epochs, losses)\n",
    "plt.plot(epochs, weights)\n",
    "\n",
    "# prediction after training\n",
    "print('Prediction after training: x=5, y=' + str(w*5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop with PyTorch\n",
    "for the backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: x=5, y=0.0\n",
      "epoch: 2, w: 1.5197999477386475, loss: 24.49020004272461\n",
      "epoch: 4, w: 1.8847039937973022, loss: 1.4118115901947021\n",
      "epoch: 6, w: 1.9723174571990967, loss: 0.0813881903886795\n",
      "epoch: 8, w: 1.9933533668518066, loss: 0.004691871348768473\n",
      "epoch: 10, w: 1.9984041452407837, loss: 0.0002704716462176293\n",
      "epoch: 12, w: 1.9996168613433838, loss: 1.5594378055538982e-05\n",
      "epoch: 14, w: 1.9999079704284668, loss: 8.987138926386251e-07\n",
      "epoch: 16, w: 1.999977946281433, loss: 5.1823555224927986e-08\n",
      "epoch: 18, w: 1.9999946355819702, loss: 3.004425153108059e-09\n",
      "epoch: 20, w: 1.999998688697815, loss: 1.7278267705478356e-10\n",
      "Prediction after training: x=5, y=9.999993324279785\n"
     ]
    }
   ],
   "source": [
    "# linear regression so - function = weight * sample\n",
    "\n",
    "# initialize our training samples\n",
    "X = torch.tensor([1,2,3,4,5,6,7,8], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8,10,12,14,16], dtype=torch.float32)\n",
    "\n",
    "# initialize our weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# calculate model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# calculate loss - mean squared error\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# prediction before training\n",
    "print(f'Prediction before training: x=5, y={forward(5).item()}')\n",
    "\n",
    "# training the model\n",
    "lr = 0.01\n",
    "n_iters = 20\n",
    "epochs = []\n",
    "losses = []\n",
    "weights = []\n",
    "for epoch in range(1, n_iters+1):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss(Y, y_pred)\n",
    "    epochs.append(epoch)\n",
    "    losses.append(l.item)\n",
    "\n",
    "    # update the weights\n",
    "    l.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "\n",
    "    # set the gradients to 0 again\n",
    "    weights.append(w.item)\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    # update the epochs\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch: {epoch}, w: {w}, loss: {l}')\n",
    "\n",
    "# prediction after training\n",
    "print(f'Prediction after training: x=5, y={forward(5).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Pipeline with PyTorch\n",
    "Using more pytorch modules including the optimizer and loss calculation<br>\n",
    "In training a model with PyTorch, use the following steps: <br>\n",
    "1. design the model (input size, output size, forward pass)\n",
    "2. construct the loss and optimizer\n",
    "3. create the training loop \n",
    "    1. forward pass...compute the prediction\n",
    "    2. backward pass...gradients\n",
    "    3. update the weights...to increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: x=5, y=0.001\n",
      "epoch: 10, w: 1.657, loss: 1.198\n",
      "epoch: 20, w: 1.908, loss: 0.034\n",
      "epoch: 30, w: 1.950, loss: 0.003\n",
      "epoch: 40, w: 1.958, loss: 0.002\n",
      "epoch: 50, w: 1.960, loss: 0.002\n",
      "epoch: 60, w: 1.961, loss: 0.002\n",
      "epoch: 70, w: 1.963, loss: 0.002\n",
      "epoch: 80, w: 1.964, loss: 0.002\n",
      "epoch: 90, w: 1.965, loss: 0.002\n",
      "epoch: 100, w: 1.966, loss: 0.002\n",
      "Prediction after training: x=5, y=9.829\n"
     ]
    }
   ],
   "source": [
    "# a new package\n",
    "import torch.nn as nn # a neural network module\n",
    "\n",
    "# linear regression so - function = weight * sample\n",
    "\n",
    "# initialize our training samples\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "XTest = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# get the size of input\n",
    "n_samples, n_features = X.size()\n",
    "\n",
    "inputSize = n_features\n",
    "outputsize = n_features\n",
    "\n",
    "# initialize the model\n",
    "model = nn.Linear(inputSize, outputsize)\n",
    "\n",
    "# prediction before training\n",
    "print(f'Prediction before training: x=5, y={model(XTest).item():.3f}')\n",
    "\n",
    "# training the model\n",
    "lr = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# define the loss and optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "for epoch in range(1, n_iters+1):\n",
    "    # forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # update the weights\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set the gradients to 0 again\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # update the epochs\n",
    "    if epoch % 10 == 0:\n",
    "        w, b = model.parameters()\n",
    "        print(f'epoch: {epoch}, w: {w[0][0]:.3f}, loss: {l:.3f}')\n",
    "\n",
    "# prediction after training\n",
    "print(f'Prediction after training: x=5, y={forward(5).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018424330fd2776b878de25387691c469f53901b81fab446e5c14eecfc2492a5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
